{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (a) Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_line(line):\n",
    "    sentence, score = line.split('\\t')\n",
    "    return [sentence.strip(), int(score.strip())]\n",
    "\n",
    "def read_data(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    return map(process_line, lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_data = pd.DataFrame(read_data('sentiment_labelled_sentences/amazon_cells_labelled.txt'), columns=['sentence', 'score'])\n",
    "imdb_data = pd.DataFrame(read_data('sentiment_labelled_sentences/imdb_labelled.txt'), columns=['sentence', 'score'])\n",
    "yelp_data = pd.DataFrame(read_data('sentiment_labelled_sentences/yelp_labelled.txt'), columns=['sentence', 'score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "500\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "print amazon_data['score'].sum()\n",
    "print imdb_data['score'].sum()\n",
    "print yelp_data['score'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([amazon_data, imdb_data, yelp_data], keys=['amazon', 'imdb', 'yelp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (b) Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/lifei/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/lifei/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/lifei/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import contractions\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess(sentence):\n",
    "    sentence = contractions.fix(sentence)  # expand contractions\n",
    "    sentence = sentence.strip().lower().translate(None, string.punctuation)  # lowercase and strip punctuations\n",
    "    sentence = re.sub(r'\\d+', '', sentence)  # remove numbers\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    words = word_tokenize(sentence)\n",
    "    wnl = WordNetLemmatizer()\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    words = [wnl.lemmatize(stemmer.stem(word.strip().decode('utf-8'))) for word in words if word not in stop_words] # remove stopwords, stemming and lemmatization\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['sentence'] = map(preprocess, dataset['sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (c) Split training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pd.concat([dataset.loc['amazon'].loc[dataset.loc['amazon']['score'] == 0].iloc[:400],\n",
    "                          dataset.loc['amazon'].loc[dataset.loc['amazon']['score'] == 1].iloc[:400],\n",
    "                          dataset.loc['imdb'].loc[dataset.loc['imdb']['score'] == 0].iloc[:400],\n",
    "                          dataset.loc['imdb'].loc[dataset.loc['imdb']['score'] == 1].iloc[:400],\n",
    "                          dataset.loc['yelp'].loc[dataset.loc['yelp']['score'] == 0].iloc[:400],\n",
    "                          dataset.loc['yelp'].loc[dataset.loc['yelp']['score'] == 1].iloc[:400]])\n",
    "testing_set = pd.concat([dataset.loc['amazon'].loc[dataset.loc['amazon']['score'] == 0].iloc[400:],\n",
    "                          dataset.loc['amazon'].loc[dataset.loc['amazon']['score'] == 1].iloc[400:],\n",
    "                          dataset.loc['imdb'].loc[dataset.loc['imdb']['score'] == 0].iloc[400:],\n",
    "                          dataset.loc['imdb'].loc[dataset.loc['imdb']['score'] == 1].iloc[400:],\n",
    "                          dataset.loc['yelp'].loc[dataset.loc['yelp']['score'] == 0].iloc[400:],\n",
    "                          dataset.loc['yelp'].loc[dataset.loc['yelp']['score'] == 1].iloc[400:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (d) Bag of Words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW_dict size: 3504\n"
     ]
    }
   ],
   "source": [
    "def generate_BoW_dict(training_set):\n",
    "    BoW_dict = {}\n",
    "    tot = 0\n",
    "    for index, row in training_set.iterrows():\n",
    "        for word in row['sentence'].split(' '):\n",
    "            if word not in BoW_dict:\n",
    "                BoW_dict[word] = tot\n",
    "                tot += 1\n",
    "    print 'BoW_dict size: %d' % tot\n",
    "    return BoW_dict\n",
    "\n",
    "BoW_dict = generate_BoW_dict(training_set)\n",
    "\n",
    "def generate_BoW_feature_vector(sentence):\n",
    "    feature_vector = [0] * len(BoW_dict)\n",
    "    for word in sentence.split(' '):\n",
    "        if word in BoW_dict:\n",
    "            feature_vector[BoW_dict[word]] += 1\n",
    "    return feature_vector\n",
    "\n",
    "training_set['feature_vector'] = training_set['sentence'].apply(generate_BoW_feature_vector)\n",
    "testing_set['feature_vector'] = testing_set['sentence'].apply(generate_BoW_feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence                               way plug u unless go convert\n",
      "score                                                             0\n",
      "feature_vector    [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "Name: 0, dtype: object\n",
      "sentence              tie charger convers last minutesmajor problem\n",
      "score                                                             0\n",
      "feature_vector    [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...\n",
      "Name: 3, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print training_set.iloc[0]\n",
    "print training_set.iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (e) Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def log_norm(feature_vector):\n",
    "    # log normalization\n",
    "    return map(lambda x: math.log(x + 1.0), feature_vector)\n",
    "\n",
    "def l1_norm(feature_vector):\n",
    "    # l1 normalization\n",
    "    vector_sum = sum(feature_vector)\n",
    "    if vector_sum == 0:\n",
    "        return map(float, feature_vector)\n",
    "    return map(lambda x: x / float(vector_sum), feature_vector)\n",
    "\n",
    "def l2_norm(feature_vector):\n",
    "    # l2 normalization\n",
    "    vector_l2_norm = norm(feature_vector)\n",
    "    if vector_l2_norm == 0:\n",
    "        return map(float, feature_vector)\n",
    "    return map(lambda x: x / float(vector_l2_norm), feature_vector)\n",
    "\n",
    "training_set['feature_vector'] = training_set['feature_vector'].apply(log_norm)\n",
    "testing_set['feature_vector'] = testing_set['feature_vector'].apply(log_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence                               way plug u unless go convert\n",
      "score                                                             0\n",
      "feature_vector    [0.69314718056, 0.69314718056, 0.69314718056, ...\n",
      "Name: 0, dtype: object\n",
      "sentence              tie charger convers last minutesmajor problem\n",
      "score                                                             0\n",
      "feature_vector    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.69314718056, ...\n",
      "Name: 3, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print training_set.iloc[0]\n",
    "print training_set.iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (f) Sentiment prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression ACC: 0.8150\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array(training_set['feature_vector'].values.tolist())\n",
    "y_train = training_set['score'].astype(int)\n",
    "X_test = np.array(testing_set['feature_vector'].values.tolist())\n",
    "y_test = testing_set['score'].astype(int)\n",
    "clf_log = LogisticRegression()\n",
    "clf_log.fit(X_train, y_train)\n",
    "acc_log = clf_log.score(X_test, y_test)\n",
    "print 'Logistic Regression ACC: %.4f' % acc_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_words_importance(coef, BoW_dict):\n",
    "    importance_dict = {}\n",
    "    for word in BoW_dict:\n",
    "        importance_dict[word] = coef[BoW_dict[word]]\n",
    "    return importance_dict\n",
    "\n",
    "def print_top_20(importance_dict):\n",
    "    sorted_importance_dict = sorted(importance_dict.items(), key=lambda kv: kv[1])\n",
    "    print 'Negative top 20:'\n",
    "    print \"\\n\".join(map(lambda x: ', '.join(map(str, x)), sorted_importance_dict[:20]))\n",
    "    print ''\n",
    "    print 'Positive top 20:'\n",
    "    print \"\\n\".join(map(lambda x: ', '.join(map(str, x)), sorted_importance_dict[-1:-21:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative top 20:\n",
      "bad, -2.940010177795963\n",
      "poor, -2.5000662875487514\n",
      "worst, -2.1129273163378746\n",
      "terribl, -1.9187343968442017\n",
      "wast, -1.8419838593632043\n",
      "slow, -1.69722945329617\n",
      "suck, -1.652177338313711\n",
      "aw, -1.6272464373236337\n",
      "disappoint, -1.6269823601582507\n",
      "horribl, -1.4969783123327889\n",
      "stupid, -1.4604472244941435\n",
      "start, -1.4338083724143165\n",
      "bland, -1.4196933573475197\n",
      "fail, -1.3418801400134233\n",
      "piec, -1.341395709404752\n",
      "plot, -1.3377730434028574\n",
      "rude, -1.3156337005568535\n",
      "avoid, -1.2928377131075706\n",
      "hear, -1.2855568225651768\n",
      "hate, -1.2530753973876092\n",
      "\n",
      "Positive top 20:\n",
      "great, 3.728217853875945\n",
      "love, 3.1307304894746175\n",
      "excel, 2.543477888360755\n",
      "delici, 2.3579199440102383\n",
      "nice, 2.2610563814106817\n",
      "amaz, 2.1328363185656607\n",
      "fantast, 2.0145059778780205\n",
      "beauti, 1.9685936229207004\n",
      "awesom, 1.9095860263657145\n",
      "best, 1.887332235856134\n",
      "good, 1.8841164784088518\n",
      "perfect, 1.7982309244889034\n",
      "comfort, 1.7010272166626892\n",
      "wonder, 1.5434490656079216\n",
      "well, 1.5178364156388924\n",
      "happi, 1.4414300528592643\n",
      "incred, 1.4282204915691472\n",
      "fine, 1.3881251478268803\n",
      "funni, 1.3345018176751962\n",
      "sturdi, 1.3205563559302762\n"
     ]
    }
   ],
   "source": [
    "print_top_20(generate_words_importance(clf_log.coef_[0], BoW_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes ACC: 0.6317\n",
      "Bernoulli Naive Bayes ACC: 0.8050\n"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "bnb = BernoulliNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "bnb.fit(X_train, y_train)\n",
    "acc_gnb = gnb.score(X_test, y_test)\n",
    "acc_bnb = bnb.score(X_test, y_test)\n",
    "print 'Gaussian Naive Bayes ACC: %.4f' % acc_gnb\n",
    "print 'Bernoulli Naive Bayes ACC: %.4f' % acc_bnb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (g) N-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW_dict_2_gram size: 10789\n"
     ]
    }
   ],
   "source": [
    "def generate_BoW_dict_2_gram(training_set):\n",
    "    BoW_dict_2_gram = {}\n",
    "    tot = 0\n",
    "    for index, row in training_set.iterrows():\n",
    "        words = row['sentence'].split(' ')\n",
    "        for i in range(len(words) - 1):\n",
    "            gram = words[i] + ' ' + words[i + 1]\n",
    "            if gram not in BoW_dict_2_gram:\n",
    "                BoW_dict_2_gram[gram] = tot\n",
    "                tot += 1\n",
    "    print 'BoW_dict_2_gram size: %d' % tot\n",
    "    return BoW_dict_2_gram\n",
    "\n",
    "BoW_dict_2_gram = generate_BoW_dict_2_gram(training_set)\n",
    "\n",
    "def generate_BoW_2_gram_feature_vector(sentence):\n",
    "    feature_vector = [0] * len(BoW_dict_2_gram)\n",
    "    words = sentence.split(' ')\n",
    "    for i in range(len(words) - 1):\n",
    "        gram = words[i] + ' ' + words[i + 1]\n",
    "        if gram in BoW_dict_2_gram:\n",
    "            feature_vector[BoW_dict_2_gram[gram]] += 1\n",
    "    return feature_vector\n",
    "\n",
    "training_set['2_gram_feature_vector'] = training_set['sentence'].apply(generate_BoW_2_gram_feature_vector)\n",
    "testing_set['2_gram_feature_vector'] = testing_set['sentence'].apply(generate_BoW_2_gram_feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence                                      way plug u unless go convert\n",
      "score                                                                    0\n",
      "feature_vector           [0.69314718056, 0.69314718056, 0.69314718056, ...\n",
      "2_gram_feature_vector    [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "Name: 0, dtype: object\n",
      "sentence                     tie charger convers last minutesmajor problem\n",
      "score                                                                    0\n",
      "feature_vector           [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.69314718056, ...\n",
      "2_gram_feature_vector    [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, ...\n",
      "Name: 3, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print training_set.iloc[0]\n",
    "print training_set.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set['2_gram_feature_vector'] = training_set['2_gram_feature_vector'].apply(log_norm)\n",
    "testing_set['2_gram_feature_vector'] = testing_set['2_gram_feature_vector'].apply(log_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence                                      way plug u unless go convert\n",
      "score                                                                    0\n",
      "feature_vector           [0.69314718056, 0.69314718056, 0.69314718056, ...\n",
      "2_gram_feature_vector    [0.69314718056, 0.69314718056, 0.69314718056, ...\n",
      "Name: 0, dtype: object\n",
      "sentence                     tie charger convers last minutesmajor problem\n",
      "score                                                                    0\n",
      "feature_vector           [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.69314718056, ...\n",
      "2_gram_feature_vector    [0.0, 0.0, 0.0, 0.0, 0.0, 0.69314718056, 0.693...\n",
      "Name: 3, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print training_set.iloc[0]\n",
    "print training_set.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression ACC: 0.6433\n"
     ]
    }
   ],
   "source": [
    "X_train_2_gram = np.array(training_set['2_gram_feature_vector'].values.tolist())\n",
    "y_train_2_gram = training_set['score'].astype(int)\n",
    "X_test_2_gram = np.array(testing_set['2_gram_feature_vector'].values.tolist())\n",
    "y_test_2_gram = testing_set['score'].astype(int)\n",
    "clf_log = LogisticRegression()\n",
    "clf_log.fit(X_train_2_gram, y_train_2_gram)\n",
    "acc_log = clf_log.score(X_test_2_gram, y_test_2_gram)\n",
    "print 'Logistic Regression ACC: %.4f' % acc_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative top 20:\n",
      "wast time, -1.6266337663157204\n",
      "wast money, -1.2108755181402313\n",
      "custom servic, -0.885446698216792\n",
      "poor qualiti, -0.8081575997816369\n",
      "stay away, -0.80332538188543\n",
      "piec junk, -0.7777574898399167\n",
      "worst ever, -0.7631486261640384\n",
      "bad film, -0.7557103236968924\n",
      "realli bad, -0.7313689428304294\n",
      "wait wait, -0.7110396622789038\n",
      "good way, -0.7076382278339408\n",
      "make mistak, -0.7071255012510449\n",
      "buy product, -0.7048544951320117\n",
      "ever go, -0.6881550537396549\n",
      "zero star, -0.6663238958811999\n",
      "act bad, -0.6662888020728832\n",
      "go back, -0.6625560941137096\n",
      "anytim soon, -0.6614309848132359\n",
      "look good, -0.6601001979350706\n",
      "send back, -0.6530665613641474\n",
      "\n",
      "Positive top 20:\n",
      "work great, 2.054405466821827\n",
      "high recommend, 1.7524930364450328\n",
      "one best, 1.4582411255582608\n",
      "great phone, 1.289089164994326\n",
      "great product, 1.1872160667930691\n",
      "food good, 1.0734850744325828\n",
      "realli good, 1.0679433092586532\n",
      "easi use, 1.009555329850475\n",
      "great food, 0.9843961434373667\n",
      "reason price, 0.9029096380166195\n",
      "food delici, 0.8999181388557159\n",
      "good price, 0.8854444828109942\n",
      "great servic, 0.8851646225386762\n",
      "love place, 0.8686200339604135\n",
      "work fine, 0.835652593585165\n",
      "pretti good, 0.8017189465143757\n",
      "well made, 0.7951033012950273\n",
      "great film, 0.7902639845663137\n",
      "good product, 0.7767482726210645\n",
      "great place, 0.776207877651663\n"
     ]
    }
   ],
   "source": [
    "print_top_20(generate_words_importance(clf_log.coef_[0], BoW_dict_2_gram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes ACC: 0.6300\n",
      "Bernoulli Naive Bayes ACC: 0.6400\n"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "bnb = BernoulliNB()\n",
    "gnb.fit(X_train_2_gram, y_train_2_gram)\n",
    "bnb.fit(X_train_2_gram, y_train_2_gram)\n",
    "acc_gnb = gnb.score(X_test_2_gram, y_test_2_gram)\n",
    "acc_bnb = bnb.score(X_test_2_gram, y_test_2_gram)\n",
    "print 'Gaussian Naive Bayes ACC: %.4f' % acc_gnb\n",
    "print 'Bernoulli Naive Bayes ACC: %.4f' % acc_bnb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (h) PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://en.wikipedia.org/wiki/Principal_component_analysis#Computing_PCA_using_the_covariance_method\n",
    "\n",
    "def pca(X, d):\n",
    "    # X is a matrix (n examples * p features), reduce to d = [d1, d2, ...] dimensions\n",
    "    X = np.array(X)\n",
    "    n, p = X.shape\n",
    "    \n",
    "    # Calculate emperical mean\n",
    "    e_mean = X.mean(axis=0)\n",
    "    \n",
    "    # Subtract emperical mean\n",
    "    B = X - e_mean\n",
    "    \n",
    "    # Calculate covariance matrix\n",
    "    C = B.T.dot(B) / (n - 1)\n",
    "    \n",
    "    # Find eigenvectors and eigenvalues of C\n",
    "    e_values, e_vectors = np.linalg.eig(C)\n",
    "    \n",
    "    # Rearrange the eigenvectors and eigenvalues\n",
    "    idx = np.argsort(e_values)[::-1]\n",
    "    e_values = e_values[idx]\n",
    "    e_vectors = e_vectors[:, idx]\n",
    "    \n",
    "    return map(lambda dd: (B.dot(e_vectors[:, :dd]), e_mean, e_values[:dd], e_vectors[:, :dd]), d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca_results = pca(X_train, [10, 50, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:1: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:2: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "X_train_pca = map(lambda x: x[0].astype(float), X_train_pca_results)\n",
    "X_test_pca = map(lambda x: (X_test - x[1]).dot(x[3]).astype(float), X_train_pca_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression ACC: 0.5867\n",
      "Logistic Regression ACC: 0.6983\n",
      "Logistic Regression ACC: 0.7333\n"
     ]
    }
   ],
   "source": [
    "def LR(X_train, X_test, y_train, y_test):\n",
    "    clf_log = LogisticRegression()\n",
    "    clf_log.fit(X_train, y_train)\n",
    "    acc_log = clf_log.score(X_test, y_test)\n",
    "    print 'Logistic Regression ACC: %.4f' % acc_log\n",
    "    return acc_log\n",
    "\n",
    "import functools\n",
    "acc_log_10, acc_log_50, acc_log_100 = map(functools.partial(LR, y_train=y_train, y_test=y_test), X_train_pca, X_test_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes ACC: 0.5867\n",
      "Bernoulli Naive Bayes ACC: 0.5650\n",
      "Gaussian Naive Bayes ACC: 0.6300\n",
      "Bernoulli Naive Bayes ACC: 0.6233\n",
      "Gaussian Naive Bayes ACC: 0.6383\n",
      "Bernoulli Naive Bayes ACC: 0.6650\n"
     ]
    }
   ],
   "source": [
    "def NB(X_train, X_test, y_train, y_test):\n",
    "    gnb = GaussianNB()\n",
    "    bnb = BernoulliNB()\n",
    "    gnb.fit(X_train, y_train)\n",
    "    bnb.fit(X_train, y_train)\n",
    "    acc_gnb = gnb.score(X_test, y_test)\n",
    "    acc_bnb = bnb.score(X_test, y_test)\n",
    "    print 'Gaussian Naive Bayes ACC: %.4f' % acc_gnb\n",
    "    print 'Bernoulli Naive Bayes ACC: %.4f' % acc_bnb\n",
    "    return acc_gnb, acc_bnb\n",
    "\n",
    "import functools\n",
    "acc_NB_10, acc_NB_50, acc_NB_100 = map(functools.partial(NB, y_train=y_train, y_test=y_test), X_train_pca, X_test_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2_gram_pca_results = pca(X_train_2_gram, [10, 50, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:1: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:2: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "X_train_2_gram_pca = map(lambda x: x[0].astype(float), X_train_2_gram_pca_results)\n",
    "X_test_2_gram_pca = map(lambda x: (X_test_2_gram - x[1]).dot(x[3]).astype(float), X_train_2_gram_pca_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression ACC: 0.5050\n",
      "Logistic Regression ACC: 0.5267\n",
      "Logistic Regression ACC: 0.5367\n"
     ]
    }
   ],
   "source": [
    "acc_log_10, acc_log_50, acc_log_100 = map(functools.partial(LR, y_train=y_train_2_gram, y_test=y_test_2_gram), X_train_2_gram_pca, X_test_2_gram_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes ACC: 0.4967\n",
      "Bernoulli Naive Bayes ACC: 0.5033\n",
      "Gaussian Naive Bayes ACC: 0.5067\n",
      "Bernoulli Naive Bayes ACC: 0.5150\n",
      "Gaussian Naive Bayes ACC: 0.5133\n",
      "Bernoulli Naive Bayes ACC: 0.5283\n"
     ]
    }
   ],
   "source": [
    "acc_NB_10, acc_NB_50, acc_NB_100 = map(functools.partial(NB, y_train=y_train_2_gram, y_test=y_test_2_gram), X_train_2_gram_pca, X_test_2_gram_pca)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
